{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module for Image Segmentation and Ruler Calibration\n",
    "\n",
    "This script performs image segmentation on images located in a specified input folder using the SAM (Segment Anything Model).\n",
    "It also optionally calibrates images by detecting a pixel-to-millimeter ruler using OCR (pytesseract) on predefined phrases/lengths.\n",
    "\n",
    "The processing pipeline includes:\n",
    "    1. Preprocessing:\n",
    "       - Crops the staging area from each image using edge detection and morphological operations.\n",
    "       - Converts images to grayscale, applies Gaussian blur, Sobel edge detection, and morphological closing/opening.\n",
    "    2. Mask Generation:\n",
    "       - Uses the SAM model to generate segmentation masks.\n",
    "       - Saves intermediate binary masks (if save_intermediate is True) for the raw SAM output and after area filtering.\n",
    "    3. Mask Filtering:\n",
    "       - Applies an area threshold to remove large background objects.\n",
    "       - Applies a mean pixel value threshold to exclude low-mean-value objects (e.g., water).\n",
    "    4. Ruler Calibration (Optional):\n",
    "       - If calibrateRuler is enabled, calibrates the image by detecting either a specific text phrase or two numeric words on a ruler.\n",
    "       - Uses pytesseract for OCR and computes the pixels-per-millimeter ratio.\n",
    "       - Outputs calibration results to a text file.\n",
    "    5. Annotation and Saving:\n",
    "       - Overlays the final binary mask on the original image.\n",
    "       - Draws the detection line (from calibration) on the image if available.\n",
    "       - Saves cropped, annotated images and all intermediate outputs to designated sub-folders within the input folder.\n",
    "\n",
    "Input Parameters:\n",
    "    - input_folder (str): Folder containing original images. Output folders will be created within this folder.\n",
    "    - save_intermediate (bool): Flag to save intermediate masks (raw SAM output and post-area threshold).\n",
    "    - SAM Model Parameters:\n",
    "        - model_type (str): Type of SAM model to use ('vit_t', 'vit_b', 'vit_l', or 'vit_h').\n",
    "        - checkpoint_path (str): Path to the SAM model checkpoint.\n",
    "    - Segmentation Thresholds:\n",
    "        - mean_threshold (int): Minimum mean pixel value for including an object.\n",
    "        - area_threshold (float): Relative area threshold to exclude large background objects.\n",
    "    - Ruler Calibration Parameters:\n",
    "        - calibrateRuler (bool): Whether to perform pixel/mm calibration using a ruler in the image.\n",
    "        - pyTesseractPath (str): Path to the Tesseract executable.\n",
    "        - useTextMode (bool): Mode for calibration. If True, searches for a text phrase; if False, searches for two numeric values such as on a ruler.\n",
    "        - phrase (str): Text phrase (or two numbers separated by space in numeric mode) to detect on the ruler.\n",
    "        - phrase_length_mm (float): The known physical length (in mm) of the phrase or spacing between numbers.\n",
    "\n",
    "Module Dependencies:\n",
    "    - OpenCV (cv2), NumPy, os, and torch.\n",
    "    - PIL and IPython for image display.\n",
    "    - pytesseract and tesseract for OCR (when ruler calibration is enabled). Follow https://github.com/UB-Mannheim/tesseract/wiki for instructions. \n",
    "    - The SAM model and automatic mask generator from the mobile_sam module.\n",
    "\n",
    "Usage:\n",
    "    1. Set the input parameters and file paths at the top of the script.\n",
    "    2. Ensure that all dependencies (SAM model, pytesseract, etc.) are properly installed and configured.\n",
    "    3. Run the script. Processed images, masks, calibration results, and annotated images will be saved in sub-folders within the input folder.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "####INPUT PARAMETERS ######\n",
    "\n",
    "\n",
    "# Input folder containing original images, output folder will be created within this folder\n",
    "input_folder = r\"C:\\Users\\jems1\\Documents\\UMD\\Lab\\chelseaOyster\\Black_typical\"  # Replace with your input folder\n",
    "\n",
    "# Set level of output verbosity\n",
    "save_intermediate = False #save intermediate masks from the 3 steps. \n",
    "\n",
    "\n",
    "\n",
    "###### IMAGE SEGMENTATION PARAMETERS ######\n",
    "# SAM Model parameters/settings\n",
    "model_type = \"vit_t\"  # Change this to 'vit_b', 'vit_l', 'vit_h', or 'vit_t' based on your use case\n",
    "checkpoint_path = r\"C:\\Users\\jems1\\Documents\\UMD\\Lab\\chelseaOyster\\checkPts\\mobile_sam.pt\"  # Replace with your checkpoint path\n",
    "\n",
    "# Thresholds for object filtering\n",
    "mean_threshold = 15  # Mean pixel value threshold for excluding low-mean-value objects (such as water)\n",
    "area_threshold = 0.5  # Relative size threshold to exclude large background objects\n",
    "##### END IMAGE SEGMENTATION PARAMETERS ######\n",
    "\n",
    "\n",
    "\n",
    "###### RULER CALIBRATION PARAMETERS ######\n",
    "# Do we want to calibrate the px/mm ruler? If True, the ruler will be detected and the user will be prompted to enter the px/mm value\n",
    "calibrateRuler = True\n",
    "pyTesseractPath = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'  # Replace with your pytesseract path\n",
    "\n",
    "useTextMode = True ##do we use numeric or text mode for the px/mm calibration. \n",
    "                    ###if true, will search for the phrase on the ruler to calibrate the ruler\n",
    "                    ###If false, will search for 2 numbers on ruler as defined in phrase\n",
    "\n",
    "phrase= \"THE OYSTER ALLIANCE OYSTER RULER\"\n",
    "##in text mode, will search for this text to calibrate the ruler\n",
    "##in numeric mode, will search for the numbers seperated by a space to calibrate the ruler, e.g. \"13 14\"\n",
    "\n",
    "phrase_length_mm = 105.7 \n",
    "##In text mode, is the length of the phrase to search for.  In numeric mode, is spacing between two numbers\n",
    "\n",
    "###END RULER CALIBRATION PARAMETERS ######\n",
    "\n",
    "#### END INPUT PARAMETERS ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE IMPORTS ####\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from mobile_sam import build_sam, sam_model_registry, SamAutomaticMaskGenerator\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "### END MODULE IMPORTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTION DEFINITIONS ####\n",
    "def imshow(im):\n",
    "    \"\"\"Helper function to display images, just for conveneince.\"\"\"\n",
    "    display(Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "def diamond_kernel(radius):\n",
    "    \"\"\"\n",
    "    Creates a diamond-shaped structuring element with given radius.\n",
    "    The diamond shape is for the removal of the ruler tick lines and image cleanup \n",
    "    \"\"\"\n",
    "    \n",
    "    size = 2 * radius + 1\n",
    "    kernel = np.zeros((size, size), dtype=np.uint8)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if abs(i - radius) + abs(j - radius) <= radius:\n",
    "                kernel[i, j] = 1\n",
    "    return kernel\n",
    "\n",
    "def cropstage(im):\n",
    "    \"\"\"\n",
    "    Crops the staging area from an image\n",
    "    Works when the box covers all 4 edges of the image.  \n",
    "    Tries to use edge detection to get the edge of the box, then crops the image to that edge.\n",
    "    Fills in holes from center, so has to have all 4 edges.  \n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Convert to grayscale and apply Gaussian filtering.\n",
    "    gray_im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    gray_im = cv2.GaussianBlur(gray_im, (0, 0), sigmaX=3)\n",
    "\n",
    "    # 2. Edge detection using Sobel operator. Should get the box edges\n",
    "    # Compute gradient magnitude.\n",
    "    sobel_x = cv2.Sobel(gray_im, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobel_y = cv2.Sobel(gray_im, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    # Normalize to range 0-255 and convert to uint8.\n",
    "    grad_mag = np.uint8(255 * grad_mag / grad_mag.max())\n",
    "\n",
    "    # Use Otsu's threshold to convert gradient magnitude to binary edge image. 2&3 equivalte matlab edge()\n",
    "    _, edge_im = cv2.threshold(grad_mag, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # 3. Morphological closing with a diamond-shaped kernel of radius 20.\n",
    "    diamond = diamond_kernel(20)\n",
    "    edge_im_closed = cv2.morphologyEx(edge_im, cv2.MORPH_CLOSE, diamond)\n",
    "    \n",
    "    \n",
    "    # 4. Fill holes, by things that don't touch the edge\n",
    "    # Approach: flood-fill from the border to find the background, invert it, then combine.\n",
    "    im_floodfill = edge_im_closed.copy()\n",
    "    h, w = im_floodfill.shape[:2]\n",
    "    mask = np.zeros((h + 2, w + 2), np.uint8)\n",
    "    cv2.floodFill(im_floodfill, mask, (0, 0), 255)\n",
    "    # Invert floodfilled image to get the holes.\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    "    # Combine to fill holes.\n",
    "    big_obj = edge_im_closed | im_floodfill_inv\n",
    "    #imshow(big_obj)\n",
    "    # 5. Keep only the largest connected component.\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(big_obj, connectivity=8)\n",
    "    if num_labels > 1:\n",
    "        # Ignore background (label 0) and find the largest component.\n",
    "        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "        big_obj = np.zeros_like(big_obj)\n",
    "        big_obj[labels == largest_label] = 255\n",
    "    else:\n",
    "        big_obj = np.zeros_like(big_obj)\n",
    "\n",
    "    # 6. Morphological opening with a disk-shaped kernel (radius 50).  Cause big kernel for small object removal\n",
    "    disk_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * 50 + 1, 2 * 50 + 1))\n",
    "    big_obj = cv2.morphologyEx(big_obj, cv2.MORPH_OPEN, disk_kernel)\n",
    "\n",
    "    # 7. Find the bounding box of the largest object to crop the stage area.\n",
    "    contours, _ = cv2.findContours(big_obj, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        # Select the largest contour by area.\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w_box, h_box = cv2.boundingRect(largest_contour)\n",
    "        # Crop the stage area from the original image.\n",
    "        stage = im[y:y+h_box, x:x+w_box]\n",
    "    else:\n",
    "        stage = im.copy()\n",
    "    #display(Image.fromarray(cv2.cvtColor(stage, cv2.COLOR_BGR2RGB)))\n",
    "    return stage\n",
    "\n",
    "def calibrate_image(\n",
    "    img,\n",
    "    useTextMode: bool = True,\n",
    "    # Oyster text parameters\n",
    "    phrase: str = \"THE OYSTER ALLIANCE OYSTER RULER\",\n",
    "    phrase_length_mm: float = 105.7,\n",
    "    # Tesseract config\n",
    "    tesseract_config: str = \"--psm 11\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects either:\n",
    "      1) The Oyster Alliance text bounding box (if useTextMode=True), or\n",
    "      2) The distance between two numeric words (e.g., '13' and '14').\n",
    "    Returns:\n",
    "      pxPerMM (float or None):  Pixels per mm if calibration was successful, else None.\n",
    "      detection_info (dict):    Details of the detection, including bounding boxes or midpoints,\n",
    "                                plus a 'detection_line_pts' that shows how we measured the text.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale and threshold (invert + Otsu)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # For numeric mode, remove thin lines with morphological open, this is likely to be the ruler ticks\n",
    "    if not useTextMode:\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    # OCR\n",
    "    ocr_data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=tesseract_config)\n",
    "\n",
    "    pxPerMM = None\n",
    "    detection_info = {\n",
    "        \"mode\": \"text_mode\" if useTextMode else \"numeric_mode\",\n",
    "        \"detection_line_pts\": None,  # We'll fill this in once we find the text\n",
    "    }\n",
    "\n",
    "    if useTextMode:\n",
    "        # --- OYSTER ALLIANCE TEXT MODE ---\n",
    "        lines = {}\n",
    "        n_boxes = len(ocr_data['text'])\n",
    "\n",
    "        for i in range(n_boxes):\n",
    "            txt = ocr_data['text'][i].strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            block_num = ocr_data['block_num'][i]\n",
    "            line_num = ocr_data['line_num'][i]\n",
    "\n",
    "            x = ocr_data['left'][i]\n",
    "            y = ocr_data['top'][i]\n",
    "            w = ocr_data['width'][i]\n",
    "            h = ocr_data['height'][i]\n",
    "\n",
    "            line_key = (block_num, line_num)\n",
    "            if line_key not in lines:\n",
    "                lines[line_key] = {\n",
    "                    \"words\": [],\n",
    "                    \"lefts\": [],\n",
    "                    \"tops\": [],\n",
    "                    \"rights\": [],\n",
    "                    \"bottoms\": []\n",
    "                }\n",
    "            lines[line_key][\"words\"].append(txt)\n",
    "            lines[line_key][\"lefts\"].append(x)\n",
    "            lines[line_key][\"tops\"].append(y)\n",
    "            lines[line_key][\"rights\"].append(x + w)\n",
    "            lines[line_key][\"bottoms\"].append(y + h)\n",
    "\n",
    "        # Search for the text phrase in each line\n",
    "        for line_key, info in lines.items():\n",
    "            line_text = \" \".join(info[\"words\"]).upper()\n",
    "            if phrase.upper() not in line_text:\n",
    "                continue\n",
    "\n",
    "            # Found the line containing the phrase\n",
    "            x1 = info[\"lefts\"][0]\n",
    "            y1 = info[\"bottoms\"][0]\n",
    "            x2 = info[\"rights\"][-1]\n",
    "            y2 = info[\"bottoms\"][-1]\n",
    "            # Measure the top line for the bounding box width\n",
    "            line_length_px = abs(x2 - x1)\n",
    "\n",
    "            # Convert to pxPerMM\n",
    "            pxPerMM = line_length_px / phrase_length_mm\n",
    "\n",
    "            # Save detection info\n",
    "            detection_info[\"bounding_box\"] = {\n",
    "                \"x1\": x1, \"y1\": y1,\n",
    "                \"x2\": x2, \"y2\": y2\n",
    "            }\n",
    "            # The detection line is the top edge of that bounding box\n",
    "            detection_info[\"detection_line_pts\"] = ((x1, y1), (x2, y2))\n",
    "\n",
    "            break  # Stop after first match\n",
    "\n",
    "    else:\n",
    "        # --- NUMERIC MODE (e.g. '13 14') ---\n",
    "        words = phrase.split()\n",
    "        if len(words) != 2:\n",
    "            raise ValueError(\"numeric_phrase must have exactly two words, e.g. '13 14'.\")\n",
    "\n",
    "        word1, word2 = words\n",
    "        found1 = None\n",
    "        found2 = None\n",
    "        n_boxes = len(ocr_data[\"text\"])\n",
    "\n",
    "        for i in range(n_boxes):\n",
    "            txt = ocr_data[\"text\"][i].strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "\n",
    "            x = ocr_data['left'][i]\n",
    "            y = ocr_data['top'][i]\n",
    "            w = ocr_data['width'][i]\n",
    "            h = ocr_data['height'][i]\n",
    "\n",
    "            if txt == word1:\n",
    "                found1 = (x, y, w, h)\n",
    "            elif txt == word2:\n",
    "                found2 = (x, y, w, h)\n",
    "\n",
    "        if found1 and found2:\n",
    "            x1, y1, w1, h1 = found1\n",
    "            mid1 = (x1 + w1 / 2.0, y1 + h1 / 2.0)\n",
    "\n",
    "            x2, y2, w2, h2 = found2\n",
    "            mid2 = (x2 + w2 / 2.0, y2 + h2 / 2.0)\n",
    "\n",
    "            dist_px = np.hypot(mid2[0] - mid1[0], mid2[1] - mid1[1])\n",
    "            pxPerMM = dist_px / phrase_length_mm\n",
    "\n",
    "            detection_info[\"found_words\"] = {word1: found1, word2: found2}\n",
    "            # The detection line is the line connecting midpoints\n",
    "            detection_info[\"detection_line_pts\"] = (\n",
    "                (int(mid1[0]), int(mid1[1])),\n",
    "                (int(mid2[0]), int(mid2[1]))\n",
    "            )\n",
    "\n",
    "    return pxPerMM, detection_info\n",
    "    \n",
    "def annotateImage(img, mask, detectionLinePoints=None):\n",
    "    ##helper function to take img, mask, and detectionLinePoints, \n",
    "    #and return annotated image that has the binary mask overlayed on the image,\n",
    "    #and the detection line drawn on the image\n",
    "    ##if detection not done or failed, skips the detection line drawing\n",
    "    \n",
    "    colored_mask = np.zeros_like(img)\n",
    "    colored_mask[:, :] = (0, 0, 255)  # Red color in BGR\n",
    "\n",
    "    # Create an overlay image by blending only in the masked areas\n",
    "    alpha = 0.5  # Transparency factor\n",
    "\n",
    "    # Make a copy of the original image to apply the overlay\n",
    "    overlaid = img.copy()\n",
    "    # For pixels where mask is non-zero, blend the original image with the colored mask\n",
    "    overlaid[mask != 0] = cv2.addWeighted(img[mask != 0], 1 - alpha, \n",
    "                                        colored_mask[mask != 0], alpha, 0)\n",
    "    \n",
    "    if detectionLinePoints is not None:\n",
    "        # Draw the detection line on the overlay image\n",
    "        (dx1, dy1), (dx2, dy2) = detectionLinePoints[\"detection_line_pts\"]\n",
    "        cv2.line(overlaid, (dx1, dy1), (dx2, dy2), (0, 255, 0), 2)\n",
    "    return overlaid\n",
    "\n",
    "### END FUNCTION DEFINITIONS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pre-loop initialization\n",
    "\n",
    "# Determine if cuda is available (for speed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Output folders for intermediate and final results, created within input folder\n",
    "if save_intermediate:\n",
    "    output_folder_step_0 = os.path.join(input_folder, \"Binary_Mask_Step_0\")  # SAM output\n",
    "    output_folder_step_1 = os.path.join(input_folder, \"Binary_Mask_Step_1\")  # After area threshold\n",
    "    os.makedirs(output_folder_step_0, exist_ok=True)\n",
    "    os.makedirs(output_folder_step_1, exist_ok=True)\n",
    "    \n",
    "if calibrateRuler:\n",
    "    calib_folder = os.path.join(input_folder, \"Calibration_Results\")  # Calibration data saved here\n",
    "    os.makedirs(calib_folder, exist_ok=True)\n",
    "    \n",
    "output_folder_step_2 = os.path.join(input_folder, \"Binary_Mask_Step_2\")  # Final processed masks\n",
    "crop_folder = os.path.join(input_folder, \"Cropped_Images\")  # Cropped images\n",
    "annotated_folder = os.path.join(input_folder, \"Annotated_Images\")  # Annotated images\n",
    "os.makedirs(output_folder_step_2, exist_ok=True)\n",
    "os.makedirs(annotated_folder, exist_ok=True)\n",
    "os.makedirs(crop_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Initialize the SAM model\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "\n",
    "# Initialize the automatic mask generator\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "if calibrateRuler:\n",
    "    import pytesseract\n",
    "    from pytesseract import Output\n",
    "    # set pytesseract path\n",
    "    pytesseract.pytesseract.tesseract_cmd = pyTesseractPath\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete for con1_2N1.jpeg\n",
      "Processing complete for con1_3N1.jpeg\n",
      "Processing complete for con2_2N1.jpeg\n",
      "Processing complete for con2_3N1.jpeg\n",
      "Processing complete for ctHp1_2N1.jpeg\n",
      "Processing complete for ctHp1_3N1.jpeg\n",
      "Processing complete for ctHp2_2N1.jpeg\n",
      "Processing complete for ctHp2_3N1.jpeg\n",
      "Processing complete for ctSp1_2N1.jpeg\n",
      "Processing complete for ctSp1_3N1.jpeg\n",
      "Processing complete for ctSp2_2N1.jpeg\n",
      "Processing complete for ctSp2_3N1.jpeg\n",
      "Processing complete for npH1_2N1.jpeg\n",
      "Processing complete for npH2_2N1.jpeg\n",
      "Processing complete for npS1_2N1.jpeg\n",
      "Processing complete for npS2_2N1.jpeg\n",
      "Processing complete for pH2_2N1.jpeg\n",
      "Processing complete for pH2_3N1.jpeg\n",
      "Processing complete for pS2_2N1.jpeg\n",
      "Processing complete for pS2_3N1.jpeg\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "#### BEGIN FILE PROCESSING LOOP ####\n",
    "# Process each file in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue\n",
    "\n",
    "    # Load the original image\n",
    "    image_path = os.path.join(input_folder, file_name)\n",
    "    rawImage = cv2.imread(image_path)\n",
    "\n",
    "    if rawImage is None:\n",
    "        print(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Crop the staging area for the image, otherwise the mask will be faulty\n",
    "    image = cropstage(rawImage)\n",
    "    \n",
    "    #write the cropped image to outputFolder\n",
    "    cv2.imwrite(os.path.join(crop_folder, file_name), image)\n",
    "    \n",
    "    # Generate masks using SAM\n",
    "    try:\n",
    "        masks = mask_generator.generate(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during mask generation for {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Step 0: Save the SAM output directly\n",
    "    if masks:\n",
    "        binary_mask_step_0 = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        for mask in masks:\n",
    "            binary_mask_step_0 = np.maximum(binary_mask_step_0, mask[\"segmentation\"].astype(np.uint8))\n",
    "        # Save the raw SAM output\n",
    "        if save_intermediate:\n",
    "            output_file_name_step_0 = f\"step_0_binary_mask_{file_name}\"\n",
    "            output_path_step_0 = os.path.join(output_folder_step_0, output_file_name_step_0)\n",
    "            cv2.imwrite(output_path_step_0, binary_mask_step_0 * 255)\n",
    "    else:\n",
    "        print(f\"No masks were generated for {file_name}.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Apply area threshold to exclude large background objects\n",
    "    binary_mask_step_1 = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "    total_pixels = image.shape[0] * image.shape[1]  # Total pixels in the image\n",
    "\n",
    "    for mask in masks:\n",
    "        segmentation = mask[\"segmentation\"].astype(np.uint8)\n",
    "        mask_area = mask[\"area\"]  # Area of the mask\n",
    "\n",
    "        if mask_area / total_pixels < area_threshold:  # Exclude large masks\n",
    "            binary_mask_step_1 = np.maximum(binary_mask_step_1, segmentation)\n",
    "\n",
    "    if save_intermediate:\n",
    "        # Save the Step 1 mask\n",
    "        output_file_name_step_1 = f\"step_1_binary_mask_{file_name}\"\n",
    "        output_path_step_1 = os.path.join(output_folder_step_1, output_file_name_step_1)\n",
    "        cv2.imwrite(output_path_step_1, binary_mask_step_1 * 255)\n",
    "\n",
    "    # Step 2: Apply mean pixel value threshold\n",
    "    binary_mask_step_2 = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    for mask in masks:\n",
    "        segmentation = mask[\"segmentation\"].astype(np.uint8)\n",
    "        mask_area = mask[\"area\"]\n",
    "        mean_pixel_value = cv2.mean(image, mask=segmentation)[0]  # Mean pixel value in the object\n",
    "\n",
    "        # Include the object if it passes both thresholds\n",
    "        if mask_area / total_pixels < area_threshold and mean_pixel_value > mean_threshold:\n",
    "            binary_mask_step_2 = np.maximum(binary_mask_step_2, segmentation)\n",
    "        \n",
    "    # Save the Step 2 mask\n",
    "    output_file_name_step_2 = f\"step_2_binary_mask_{file_name}\"\n",
    "    output_path_step_2 = os.path.join(output_folder_step_2, output_file_name_step_2)\n",
    "    cv2.imwrite(output_path_step_2, binary_mask_step_2 * 255)\n",
    "    #print(f\"Segmentation done for {file_name}\")\n",
    "    \n",
    "    if calibrateRuler:\n",
    "        pxPerMM, det_info = calibrate_image(image, useTextMode=useTextMode, phrase=phrase, phrase_length_mm=phrase_length_mm)\n",
    "\n",
    "        \n",
    "        #dump calibration information\n",
    "        \n",
    "        ##extract filename from file_name\n",
    "        calib_file_name = f\"calibration_{os.path.splitext(file_name)[0]}.txt\"\n",
    "        calib_file_path = os.path.join(calib_folder, calib_file_name)\n",
    "        \n",
    "        if pxPerMM is None:\n",
    "            ##hos img and detection info if detection failed.  \n",
    "            ##TODO maybe have an info dump of detected words annotated?  \n",
    "            print(\"Calibration failed.\")\n",
    "            print(\"Detection Info:\", det_info)\n",
    "            with open(calib_file_path, \"w\") as f:\n",
    "                f.write(\"Calibration failed.\\n\")\n",
    "                f.write(f\"Detection Info: {det_info}\\n\")\n",
    "        else:\n",
    "            with open(calib_file_path, \"w\") as f:\n",
    "                f.write(f\"pxPerMM: {pxPerMM}\\n\")\n",
    "                f.write(f\"Detection Info: {det_info}\\n\")\n",
    "        \n",
    "        annotated_img = annotateImage(image, binary_mask_step_2, det_info)\n",
    "    else:\n",
    "        annotated_img = annotateImage(image, binary_mask_step_2)\n",
    "    cv2.imwrite(os.path.join(annotated_folder, file_name), annotated_img)\n",
    "    print(f\"Processing complete for {file_name}\")\n",
    "print(\"Processing complete!\")\n",
    "#### END FILE PROCESSING LOOP ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chelseaOyster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
